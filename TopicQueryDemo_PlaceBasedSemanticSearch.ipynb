{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Topic Query Functionality - Demo\n",
    "This notebook showcases the **topic query functionality** of the place-based semantic search system, enabling users to investigate how specific themes are distributed across urban places through geosocial media signals.\n",
    "\n",
    "The approach allows free-text input, which is semantically encoded using transformer-based language models. This query embedding is then matched against a large set of precomputed embeddings derived from geotagged social media posts. To efficiently retrieve the most relevant content, the system uses **FAISS** with inner product similarity for approximate nearest-neighbor search.\n",
    "\n",
    "The results are then aggregated spatially, revealing where in the urban environment posts most closely align with the meaning of the query. Each area’s semantic relevance is visualized using **word clouds** that highlight the most frequent and meaningful terms found in matching posts.\n",
    "\n",
    "This functionality supports spatial exploration of concepts such as leisure, nature, spirituality, or any other custom user-defined theme.\n",
    "\n",
    "---\n",
    "\n",
    "**Notebook Outline**\n",
    "\n",
    "**1. Parse Data**\n",
    "\n",
    "Load post embeddings and associated metadata for spatially-aware retrieval.\n",
    "\n",
    "**2. Topic Query**\n",
    "\n",
    "Process a free-text query into a semantic embedding and perform similarity search against geotagged posts.\n",
    "\n",
    "**3. Visualization**\n",
    "\n",
    "Generate word clouds for each spatial cluster or region containing thematically relevant posts.\n",
    "\n",
    "**4.  Tests**\n",
    "\n",
    "To demonstrate the system's capabilities, we run three example queries that reflect distinct place-related themes:\n",
    "\n",
    "- **Query 1:** *recreation and outdoor activities*  \n",
    "- **Query 2:** *observing plants and animals in green spaces*  \n",
    "- **Query 3:** *peaceful sacred places for meditation, prayer, or reflection*\n",
    "\n",
    "Each example retrieves and visualizes the locations where posts most closely match the query.\n",
    "\n",
    "---\n",
    "\n",
    "**Data Availability**\n",
    "\n",
    "The original geosocial media data used in this study, comprising geotagged posts from  Instagram, Flickr, and X (formerly Twitter), cannot be publicly shared. All data utilized in this study was collected through official APIs or authorized services that, at the time of collection, explicitly prohibited the redistribution of  user-generated content in accordance with their respective terms of service (e.g., the Twitter Developer Agreement and Instagram's API Terms of Use).  While these specific agreements are no longer publicly available due to platform  ownership changes and service restructuring, their restrictions were in effect and adhered to during the data acquisition period.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {
    "id": "E2dIPgMGVXWs"
   },
   "source": [
    "**Dependencies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 46
    },
    "id": "4diGKC0ZgETX",
    "outputId": "1064627a-8878-4c4e-ffa9-e73cbec9bc0f",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "\n",
    "import torch\n",
    "from collections import Counter\n",
    "import sqlite3\n",
    "\n",
    "from sentence_transformers import SentenceTransformer,util\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from bertopic import BERTopic\n",
    "\n",
    "import h3\n",
    "import faiss\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import folium\n",
    "import matplotlib.colors as mcolors\n",
    "import folium\n",
    "from folium.plugins import MeasureControl\n",
    "from folium import IFrame, Element\n",
    "from folium.plugins import FloatImage\n",
    "from folium import Tooltip\n",
    "import base64\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "from jenkspy import JenksNaturalBreaks\n",
    "from matplotlib.colors import ListedColormap, to_hex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {
    "id": "DqY4hBnpVXWu",
    "tags": []
   },
   "source": [
    "## 1. Parse Data\n",
    "\n",
    "\n",
    "**Note**: Due to the data-sharing restrictions described above, certain parts of this notebook reference CSV input files that are not included in the repository. Consequently, code cells that load or process these files cannot be executed as-is. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E7AJsyqsVXWv",
    "outputId": "6ab22668-b764-4f89-bc0e-6d6c4d07b1d2",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# Path to your specific folder\n",
    "folder_path = './'\n",
    "INPUT = f'{folder_path}/01_Input'\n",
    "OUTPUT = f'{folder_path}/02_Output'\n",
    "\n",
    "file_name_class = 'Dresden_GSMDataset_classifiesMidGeoTextEmb_withoutOutliers.csv'\n",
    "\n",
    "db_path_posts = f'{OUTPUT}/Posts_Embeddings.db'\n",
    "df = pd.read_csv(f'{OUTPUT}/{file_name_class}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2. Semantic Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "**Helper Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_similarity_distribution(similarity_scores, query, z_factor):\n",
    "    \"\"\"\n",
    "    Plots a histogram of cosine similarity scores.\n",
    "\n",
    "    Args:\n",
    "    - similarity_scores (list): List of cosine similarity scores.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(6,3))\n",
    "    plt.hist(similarity_scores, bins=50, color='blue', alpha=0.7)\n",
    "    plt.axvline(np.mean(similarity_scores), color='red', linestyle='dashed', linewidth=2, label=\"μ\")\n",
    "    plt.axvline(np.mean(similarity_scores) + z_factor * np.std(similarity_scores), color='green', linestyle='dashed', linewidth=2, label=f\"T = μ + {z_factor}σ\")\n",
    "    plt.xlabel(\"Cosine Similarity Score\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.title(f\"Query: {query}\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def compute_z_score_threshold(cosine_similarities, z_factor=1.5):\n",
    "    \"\"\"\n",
    "    Computes a dynamic threshold using the Z-score method.\n",
    "\n",
    "    Args:\n",
    "        cosine_similarities (list): List of cosine similarity scores.\n",
    "        z_factor (float): Z-score factor (e.g., 1.5 means mean + 1.5 * std).\n",
    "\n",
    "    Returns:\n",
    "        float: The calculated similarity threshold.\n",
    "    \"\"\"\n",
    "    mean_sim = np.mean(cosine_similarities)\n",
    "    std_sim = np.std(cosine_similarities)\n",
    "    return mean_sim + (z_factor * std_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def semantic_search_faiss(input_text, db_path, df, model, z_factor=1.5, chunk_size=10000):\n",
    "    \"\"\"\n",
    "    Performs FAISS-based semantic search using precomputed embeddings from a SQLite database.\n",
    "    It applies cosine similarity scaling, calculates a dynamic threshold based on Z-score, \n",
    "    and generates a histogram of similarity scores.\n",
    "\n",
    "    Args:\n",
    "        input_text (str): The query text for semantic search.\n",
    "        db_path (str): Path to the SQLite database containing embeddings.\n",
    "        df (pd.DataFrame): DataFrame containing metadata to merge with search results.\n",
    "        model (SentenceTransformer): A pre-loaded SentenceTransformer model.\n",
    "        z_factor (float): The Z-score factor for threshold calculation.\n",
    "        chunk_size (int, optional): Number of rows to process at a time when reading from the database.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame of matched entries with their cosine similarity scores.\n",
    "    \"\"\"\n",
    "\n",
    "    # Compute query embedding\n",
    "    query_embedding = model.encode(input_text, convert_to_numpy=True)\n",
    "\n",
    "    # Ensure embedding is 768-dimensional (if necessary)\n",
    "    if len(query_embedding) < 768:\n",
    "        padding = np.zeros(768 - len(query_embedding), dtype=np.float32)\n",
    "        query_embedding = np.concatenate((query_embedding, padding))\n",
    "\n",
    "    # Compute the norm (magnitude) of the query embedding\n",
    "    query_norm = np.linalg.norm(query_embedding)\n",
    "    query_embedding = query_embedding.reshape(1, -1)\n",
    "\n",
    "    # Initialize FAISS index (dot product similarity)\n",
    "    dimension = 768\n",
    "    index = faiss.IndexFlatIP(dimension)\n",
    "\n",
    "    # Load embeddings from SQLite and add to FAISS index\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    query = \"SELECT post_guid, embedding FROM post_embeddings\"\n",
    "    embeddings, post_guids = [], []\n",
    "\n",
    "    for chunk in pd.read_sql_query(query, conn, chunksize=chunk_size):\n",
    "        chunk['embedding'] = chunk['embedding'].apply(lambda x: np.frombuffer(x, dtype=np.float32))\n",
    "        embeddings.extend(chunk['embedding'].tolist())\n",
    "        post_guids.extend(chunk['post_guid'].tolist())\n",
    "\n",
    "    conn.close()\n",
    "\n",
    "    embeddings = np.array(embeddings, dtype=np.float32)\n",
    "\n",
    "    # Compute norms (magnitudes) of stored embeddings\n",
    "    embedding_norms = np.linalg.norm(embeddings, axis=1)\n",
    "\n",
    "    # Add embeddings to FAISS index\n",
    "    index.add(embeddings)\n",
    "\n",
    "    # Perform FAISS search (dot product similarity)\n",
    "    distances, indices = index.search(query_embedding, len(post_guids))\n",
    "\n",
    "    # Extract raw scores (dot products)\n",
    "    raw_scores = [dist for dist in distances[0]]\n",
    "\n",
    "    if len(raw_scores) == 0:\n",
    "        print(\"⚠️ No results found.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # Compute cosine similarity: dot product / (query norm * embedding norm)\n",
    "    cosine_similarities = [\n",
    "        raw_scores[i] / (query_norm * embedding_norms[indices[0][i]])\n",
    "        for i in range(len(raw_scores))\n",
    "    ]\n",
    "\n",
    "    # Compute dynamic Z-score threshold\n",
    "    min_similarity = compute_z_score_threshold(cosine_similarities, z_factor)\n",
    "\n",
    "    # Plot histogram of similarity scores\n",
    "    plot_similarity_distribution(cosine_similarities,input_text, z_factor)\n",
    "\n",
    "    print(f\"🔹 Dynamic Cosine Similarity Threshold (Z-score {z_factor}): {min_similarity:.4f}\")\n",
    "\n",
    "    # Filter results based on the computed similarity threshold\n",
    "    results = [\n",
    "        (post_guids[indices[0][i]], cosine_similarities[i])\n",
    "        for i in range(len(cosine_similarities)) if cosine_similarities[i] >= min_similarity\n",
    "    ]\n",
    "\n",
    "    results_df = pd.DataFrame(results, columns=['post_guid', 'score'])\n",
    "    results = pd.merge(results_df, df, on='post_guid', how='inner')\n",
    "    \n",
    "    print(f\"✅ {len(results_df)} posts were identified as relevant to the query.\")\n",
    "    \n",
    "    \n",
    "\n",
    "    # Merge with metadata and return results\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## 3. Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Helper Functions Visualization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def h3_to_boundary(h3_index):\n",
    "    \"\"\"\n",
    "    Convert an H3 index to its boundary in (lat, lon) format.\n",
    "\n",
    "    Parameters:\n",
    "    - h3_index (str): H3 index.\n",
    "\n",
    "    Returns:\n",
    "    - boundary (list): List of (lat, lon) tuples.\n",
    "    \"\"\"\n",
    "    boundary = h3.h3_to_geo_boundary(h3_index)\n",
    "    return [(lat, lon) for lat, lon in boundary]\n",
    "\n",
    "\n",
    "def generate_wordclouds_for_results(results_df, text_column):\n",
    "    \"\"\"\n",
    "    Generates word clouds for all H3 hex bins in results_df.\n",
    "\n",
    "    Parameters:\n",
    "    - results_df (pd.DataFrame): The DataFrame with search results containing 'h3_index_9' and 'score'.\n",
    "    - text_column (str): The column in results_df containing the text lists.\n",
    "    \n",
    "    Returns:\n",
    "    - dict: A dictionary where keys are H3 indices, and values are word cloud images (PIL.Image).\n",
    "    \"\"\"\n",
    "    wordclouds_dict = {}\n",
    "\n",
    "    for h3_index in results_df['h3_index_9'].unique():\n",
    "        # Extract relevant posts for the H3 index\n",
    "        relevant_texts = results_df[results_df['h3_index_9'] == h3_index][text_column].astype(str)\n",
    "        concatenated_text = ' '.join(relevant_texts)\n",
    "\n",
    "        # Count word frequencies using Counter\n",
    "        word_counts = Counter(concatenated_text.split())\n",
    "\n",
    "        # Generate the word cloud using the word frequencies\n",
    "        wordcloud = WordCloud(\n",
    "            width=600,\n",
    "            height=300,\n",
    "            background_color='white',\n",
    "            contour_width=0.5,\n",
    "            contour_color='black',\n",
    "            relative_scaling='auto',\n",
    "            color_func=lambda *args, **kwargs: (37,52,148),\n",
    "            normalize_plurals=True,\n",
    "            repeat=False,\n",
    "            min_word_length=3\n",
    "        ).generate_from_frequencies(word_counts)\n",
    "\n",
    "        # Save the word cloud to the dictionary\n",
    "        wordclouds_dict[h3_index] = wordcloud.to_image()\n",
    "\n",
    "    return wordclouds_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def visualize_h3_grid(df, results_df, text_column,wordclouds=None):\n",
    "    \n",
    "    \"\"\"\n",
    "    Visualizes geographic H3 grid cells on a map, colored based on the chi-normalized \n",
    "    representation of topic relevance in each cell. This function uses Folium to create \n",
    "    an interactive map with polygons representing H3 grid cells and optional word cloud popups.\n",
    "\n",
    "    Key features:\n",
    "    - Computes the chi-normalized value for topic relevance across H3 grid cells.\n",
    "    - Uses Jenks Natural Breaks for data classification to define color bins.\n",
    "    - Displays tooltips with detailed statistics (e.g., total posts, topic posts, chi value).\n",
    "    - Optionally includes word clouds as popups for individual H3 grid cells.\n",
    "    - Adds a dynamic title and legend for better interpretability.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame containing overall post data with H3 indices.\n",
    "        results_df (pd.DataFrame): DataFrame containing filtered topic-related posts.\n",
    "        text_column (str): Column name for textual data (used in tooltips/word clouds).\n",
    "        wordclouds (dict, optional): A dictionary of word clouds keyed by H3 indices.\n",
    "\n",
    "    Returns:\n",
    "        folium.Map: An interactive Folium map visualizing the H3 grid cells.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Calculate the center of the map\n",
    "    center_lat = results_df['latitude'].mean()\n",
    "    center_lon = results_df['longitude'].mean()\n",
    "\n",
    "    # Initialize the Folium map\n",
    "    folium_map = folium.Map(location=[center_lat, center_lon], zoom_start=11, tiles=\"Cartodb positron\", control_scale = True)\n",
    "\n",
    "    # Calculate the total posts per H3 cell\n",
    "    total_posts_per_cell = df.groupby('h3_index_9').size()\n",
    "    topic_posts_per_cell = results_df.groupby('h3_index_9').size()\n",
    "    \n",
    "    # Chi normalization with proper handling of zeros\n",
    "    sum_exp = len(df)\n",
    "    sum_obs = topic_posts_per_cell.sum()\n",
    "    expected = total_posts_per_cell\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):  # Handle divisions gracefully\n",
    "        chi_values = ((topic_posts_per_cell * (sum_exp / sum_obs)) - expected) / np.sqrt(expected)\n",
    "        chi_values = chi_values.replace([np.inf, -np.inf], np.nan)  # Replace infinities with NaN\n",
    "        normalized_values = chi_values.dropna()  # Exclude invalid calculations\n",
    "\n",
    "        # Custom discrete colormap for chi normalization\n",
    "        custom_colors = ['#edf8b1','#c7e9b4','#7fcdbb','#225ea8','#0c2c84']\n",
    "        colormap = ListedColormap(custom_colors)\n",
    "\n",
    "    # Compute Jenks Natural Breaks\n",
    "    jenks = JenksNaturalBreaks(n_classes=5)\n",
    "    jenks.fit(normalized_values)\n",
    "    jenks_breaks = jenks.breaks_\n",
    "\n",
    "    # Iterate over unique H3 indices and visualize each cell\n",
    "    for h3_index, value in normalized_values.items():\n",
    "        boundary = h3.h3_to_geo_boundary(h3_index)\n",
    "        # Determine color based on Jenks classification\n",
    "        bin_index = np.digitize(value, jenks_breaks, right=True) - 1\n",
    "        bin_index = min(max(bin_index, 0), len(jenks_breaks) - 2)  # Ensure valid index\n",
    "        color = to_hex(colormap(bin_index / (len(jenks_breaks) - 1)))\n",
    "\n",
    "        # Retrieve counts for the tooltip\n",
    "        total_posts = total_posts_per_cell.get(h3_index, 0)\n",
    "        topic_posts = topic_posts_per_cell.get(h3_index, 0)\n",
    "\n",
    "        # Tooltip text with normalized value and counts\n",
    "        tooltip_text = (\n",
    "            f\"H3: {h3_index}<br>\"\n",
    "            f\"Total Posts: {total_posts}<br>\"\n",
    "            f\"Topic Posts: {topic_posts}<br>\"\n",
    "            f\"Chi: {value:.2f}\"\n",
    "        )\n",
    "\n",
    "        # Add a word cloud popup if applicable\n",
    "        if wordclouds and h3_index in wordclouds:\n",
    "            img = wordclouds[h3_index]\n",
    "            buf = BytesIO()\n",
    "            img.save(buf, format=\"PNG\")\n",
    "            buf.seek(0)\n",
    "            img_data = base64.b64encode(buf.getvalue()).decode()\n",
    "            popup_html = f'<img src=\"data:image/png;base64,{img_data}\" style=\"width:300px;height:150px;\">'\n",
    "            popup = folium.Popup(popup_html, max_width=300)\n",
    "        else:\n",
    "            popup = None\n",
    "\n",
    "        # Create a Folium polygon for the H3 cell\n",
    "        polygon = folium.Polygon(\n",
    "            locations=boundary,\n",
    "            color='gray',\n",
    "            fill_color=color,\n",
    "            fill_opacity=0.6,\n",
    "            weight=0.5,\n",
    "            tooltip=folium.Tooltip(tooltip_text),\n",
    "            )\n",
    "\n",
    "        if popup:\n",
    "            polygon.add_child(popup)\n",
    "\n",
    "        folium_map.add_child(polygon)\n",
    "                \n",
    "        title_html = f'''\n",
    "    <div style=\"\n",
    "        position: fixed; \n",
    "        top: 10px; left: 50px; \n",
    "        z-index:9999; font-size:14px; font-weight: bold; \n",
    "        background-color: rgba(255, 255, 255, 0.7);background-color: rgba(255, 255, 255, 0.7); padding: 5px; border: 1px solid grey;\">\n",
    "        Topic-Place Representation for Query: <i>{query}</i>\n",
    "    </div>\n",
    "'''\n",
    "        folium_map.get_root().html.add_child(folium.Element(title_html))\n",
    "        \n",
    "        # Add legend for the chi values\n",
    "        legend_html = '''\n",
    "    <div style=\"position: fixed; \n",
    "                bottom: 50px; left: 10px; width: 210px; height: 155px; \n",
    "                background-color: white; opacity: 0.8; z-index:9999; font-size:12px;\n",
    "                border:1px solid grey; padding: 10px;\">\n",
    "        <b>Topic-Place Representation</b><br>\n",
    "        <small>normalized by signed chi expectation</small><br><br>\n",
    "        <i style=\"background:#edf8b1; width:20px; height:10px; display:inline-block;\"></i> Strongly Underrepresented<br>\n",
    "        <i style=\"background:#c7e9b4; width:20px; height:10px; display:inline-block;\"></i> Underrepresented<br>\n",
    "        <i style=\"background:#7fcdbb; width:20px; height:10px; display:inline-block;\"></i> Moderately Represented<br>\n",
    "        <i style=\"background:#225ea8; width:20px; height:10px; display:inline-block;\"></i> Overrepresented<br>\n",
    "        <i style=\"background:#0c2c84; width:20px; height:10px; display:inline-block;\"></i> Strongly Overrepresented\n",
    "    </div>\n",
    "'''\n",
    "\n",
    "        folium_map.get_root().html.add_child(folium.Element(legend_html))\n",
    "\n",
    "    return folium_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 4. Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "**Define sentence_transformers model for the query**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = SentenceTransformer(\"paraphrase-multilingual-MiniLM-L12-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Semantic Query: Recreation and Outdoor Activities**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = 'recreation and outdoor activities'\n",
    "results_df_0 = semantic_search_faiss(query, db_path_posts, df, model,z_factor = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "wordclouds_0 = generate_wordclouds_for_results(results_df_0, 'tokens')\n",
    "map_outdoor = visualize_h3_grid(df, results_df_0,'tokens', wordclouds=wordclouds_0)\n",
    "map_outdoor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "map_outdoor.save(f\"./03_Visualizations/Recreation&OutdoorActivities.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "**Semantic Query 2: Observing plants and animals in green space**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = 'observing plants and animals in green spaces'\n",
    "results_df_0 = semantic_search_faiss(query, db_path_posts, df, model,z_factor = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "wordclouds_0 = generate_wordclouds_for_results(results_df_0, 'tokens')\n",
    "map_flora= visualize_h3_grid(df, results_df_0,'tokens', wordclouds=wordclouds_0)\n",
    "map_flora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "map_flora.save(f\"./03_Visualizations/Plants&Animals_Map_Results.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Semantic Query 3: peaceful sacred places for meditation, prayer, or reflection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = 'peaceful sacred places for meditation, prayer, or reflection'\n",
    "results_df_0 = semantic_search_faiss(query, db_path_posts, df, model,z_factor = 2.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "wordclouds_0 = generate_wordclouds_for_results(results_df_0, 'tokens')\n",
    "map_spir = visualize_h3_grid(df, results_df_0,'tokens', wordclouds=wordclouds_0)\n",
    "map_spir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "map_spir.save(f\"./03_Visualizations/Spirituality_Map_Results.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "!jupyter nbconvert --to html \"TopicQueryDemo_PlaceBasedSemanticSearch.ipynb\" --output \"TopicQueryDemo_PlaceBasedSemanticSearch.html\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
